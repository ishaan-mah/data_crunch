# qwen_bespoke
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
output_dir: /shared/share_mala/Ishaan/finetuned_model/LLaMA-Factory/qwen_bespoke
dataset: Bespoke-Stratos-17k
template: qwen    # tells LLaMA-Factory to use Qwenâ€™s chat formatting
stage: sft
do_train: true
finetuning_type: full
# training params
num_train_epochs: 3.0
per_device_train_batch_size: 2
gradient_accumulation_steps: 12   # adjust if fewer GPUs
learning_rate: 1e-5
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-08
logging_steps: 1
save_steps: 175
seed: 42
# hardware
bf16: true
ddp_find_unused_parameters: false
report_to: wandb
run_name: qwen_bespoke_full